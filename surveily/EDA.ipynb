{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe40cfc4-089b-42ad-9259-810e631332d6",
   "metadata": {},
   "source": [
    "# Personal Protective Equipment Dataset Explorative Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed46b96-a4cc-4e80-a193-e44716a8fe6c",
   "metadata": {},
   "source": [
    "## Imports and data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090f9f3f-5b38-4c3b-9cea-3223222dab91",
   "metadata": {},
   "source": [
    "### Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f835c271-3151-44b3-9c06-e7a8e06ed85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05153f44-bb6c-4e47-a311-8483c440489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d88034d9-4c2d-4360-a49e-30a69bc9b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe9611b-9b15-43d7-997e-2eba07f1bddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pawelgrabinski/Documents/Pet/surveily/surveily\n",
      "/Users/pawelgrabinski/Documents/Pet/surveily\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2f435b-70af-4d69-b995-888715218d01",
   "metadata": {},
   "source": [
    "### Loading image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf95f3a-193f-42e7-b0c1-1e1cb9438729",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = []\n",
    "path_to_train_images = os.path.join(os.getcwd(), 'dataset/train/images')\n",
    "for image_path in os.listdir(path_to_train_images):\n",
    "    image_size = cv2.imread(os.path.join(path_to_train_images, image_path)).shape\n",
    "    image_data.append((image_path, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6693219c-cdc9-47a6-885e-2757979bb8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = pd.DataFrame(image_data, columns=['image_name', 'dimensions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ecda02-d869-4519-98b9-92fb190c9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = image_df.assign(height=image_df.dimensions.apply(lambda x: x[0]),\n",
    "                           width=image_df.dimensions.apply(lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed6f0f-0254-40b7-90dd-3418911682cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = image_df.assign(dimensions=image_df.height.apply(str) + 'x' + image_df.width.apply(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32db37-1059-42d0-8511-b6fd1c383d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df = image_df.assign(image_name=image_df.image_name.apply(lambda x: x[:-4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16bfcc2-bceb-42b1-8a9d-18df04217bdc",
   "metadata": {},
   "source": [
    "### Loading labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b4324-98cd-405c-b163-f5423ac50d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = []\n",
    "path_to_train_labels = os.path.join(os.getcwd(), 'dataset/train/labels')\n",
    "for labels_path in os.listdir(path_to_train_labels):\n",
    "    with open(os.path.join(path_to_train_labels, labels_path)) as file:\n",
    "        labels = file.readlines()\n",
    "    labels = [line.strip().split() for line in labels]\n",
    "    labels = [[labels_path.replace('.txt',''),] + line for line in labels]\n",
    "    label_data.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20cf1b-7a68-48dc-82f5-7293ad82e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(label_data, columns=['image_name', 'class', 'x_box_center', 'y_box_center', 'box_width', 'box_height'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63335fff-ae50-4d79-a8b4-d85bb79f4631",
   "metadata": {},
   "source": [
    "### Merging data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69146f6-441d-416a-a21f-ea5c2d6c68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = labels_df.set_index('image_name').join(image_df.set_index('image_name'), on='image_name', how='left', lsuffix='_labels', rsuffix='_image', validate='many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d07fe8a-71d9-49c5-a597-64d1aa011487",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe651d-16c2-46c5-80e7-94c15eca32ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.assign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44d9b49-a385-459f-b08b-65bc15c382a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.assign(**{\n",
    "    column: full_df.loc[:, column].astype(float) for column in [\n",
    "        'x_box_center', 'y_box_center', 'box_width', 'box_height']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6d119-db8e-46ac-a4e7-87ea34b1aff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.assign(**{\n",
    "    column: full_df.loc[:, column].astype(int) for column in ['class', 'height', 'width']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1348a269-42d8-4b16-8432-2b662c17f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.assign(aspect=lambda x: x.width/x.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48865cc5-83f8-4411-9b88-d4090afd8958",
   "metadata": {},
   "source": [
    "## Explorative data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b22ad5-d002-4ca6-9591-1daa8fe40147",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2b1691-9657-43db-bc18-f9812a2d5418",
   "metadata": {},
   "source": [
    "### Balans klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c6d48-079e-4dd9-bd16-fbb62ab92b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([full_df.value_counts('class'), full_df.value_counts('class', normalize=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14ffca6-c944-4501-9e55-d118537606db",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.value_counts('class', normalize=True)[:4].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0166af9f-8679-4075-9e6e-58e5a813f1a3",
   "metadata": {},
   "source": [
    "Na początek możemy zauważyć, że zbiór jest zdecydowanie niezbalansowany pod względem klas. Klasa o indeksie `5` stanowi prawie jedną czwartą wszystkich etykiet. Mamy 3 klasy o podobnej liczebności, ale kolejne klasy są już kilkukrotnie mniej liczna, a najmniejliczne klasa stanowi niecały procent obserwacji.\n",
    "\n",
    "Póki co na tej podstawie możemy powiedzieć, że zbiór nie nadaje się do uczenia modelu bez dodatkowych kroków. Większość algorytmów podczas uczenia będzie zwracać wyniki skupione głównie na 4 najliczniejszych klasach, które stanowią ponad `77%` obserwacji w zbiorze.\n",
    "\n",
    "By skorygować te problemy można podejść do problemu za pomocą algorytmów, które nie będą wrażliwe na różną liczebność klas. Na przykład możemy podejść do problemu jako klasyfikacji \"multi-class multi-label\", więc dla każdej z nich możemy spróbować wyuczyć osobny klasyfikator. Jednak niska liczba próbek dla najmniej licznej klasy prawdopodobnie nie pozwali na osiągnięcie zadowalających wyników.\n",
    "\n",
    "W następnym kroku można zrezygnować z detekcji tych najmniej licznych klas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8a42f1-8ab8-4c4d-9eb3-f692a5a12109",
   "metadata": {},
   "source": [
    "### Wymiary i aspekt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe01432-db78-4c2d-b61b-56b91ae178d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7766-03d4-4c7d-9541-9ebdb8c872cc",
   "metadata": {},
   "source": [
    "Dalej możemy skupić się na wymiarach obrazów. Widzimy, że w zbiorze znajduje się pewna liczba małych obrazów, dla których maksimum dla obu szerokości lub wysokości z występujących jest kilkukortnie większe od minimum. Jednak widzimy, że większość z nich, bo już dla percentyla `25%`, ma wymiary conajmniej 720x1280 czyli ustandaryzowana rozdzielczość HD ready. Widzimy po maksiumum też, że istnieją obrazt większe. Problem z rozmiarem polega na tym, że część modeli może oczekiwać ustandaryzowanej rozdzielczości plików, więc wymagałoby to skalowania niektórych z nich. Jeśli zdecydujemy się skalować w dół, to tracimy część informacji, a jeśli decydujemy się skalować w górę, to mniejsze obrazy nie będą zawierały odpowiednich detali, które zawierają większe obrazy.\n",
    "\n",
    "Widzimy, że aspekt obrazów również może się znacząco różnić. To nakłada potencjalnie kolejne ograniczenie na modele, które chcemy wykorzystywać. Nawet przy założeniu skalowania części z obrazów, jeśli przeskalujemy je do innego aspektu, to cechy, które rozpoznają modele mogą zostać zdeformowane, a może mieć to istotny wpływ na ich działanie.\n",
    "\n",
    "Spróbujmy sprawdzić, jaka część zbioru ma aspekt ten sam, co HD ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c04ef6-0df5-4e76-903c-d1fc11cd9027",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[(full_df.aspect-1280/720).abs()<1e-6].image_name.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e296d-fbf0-4f3a-84cf-8099f6816094",
   "metadata": {},
   "source": [
    "Czyli mniej od całego zbioru o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5aa693-d395-4dca-8279-65fafa0d3d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.image_name.count() - full_df.loc[(full_df.aspect-1280/720).abs()<1e-6].image_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e827f19-c1b9-4938-919c-2936e519cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Jest to około  {round((full_df.image_name.count() - full_df.loc[(full_df.aspect-1280/720).abs()<1e-6].image_name.count())/full_df.image_name.count()*100,2)}% zbioru'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a97784-cf64-48ea-9ee5-5929b77cdbe5",
   "metadata": {},
   "source": [
    "Możemy rozważyć statystykę takiego zbioru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da3633e-9532-4d6b-85a8-44597c8d423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_aspect_df = full_df.loc[(full_df.aspect-1280/720).abs()<1e-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748adac4-9177-48e4-bef2-4ac9fe055786",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_aspect_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bf8e3-cee5-49c6-82c5-b7c79d8b0643",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_aspect_df.value_counts('dimensions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d94f50-76f6-4620-b115-e48f86b6b3c8",
   "metadata": {},
   "source": [
    "Widzimy, że zostały nam jedynie obrazy ze standardu HD ready i Full HD, gdzie tych drugich. Ponieważ HD ready zawiera już dużo szczegółów, a obrazów Full HD jest niewiele, to możemy postawić hipotezę, że można te drugie przeskalować w dół do HD ready i będziemy mieli jednolity pod względem wymiarów i tym samym aspektu zbiór obrazów.\n",
    "\n",
    "Sprawdźmy jeszcze, jak zmieniły się liczebności klas po tym odsianiu części obserwacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b5e1f-be4c-441d-8820-d38f97e0cee2",
   "metadata": {},
   "source": [
    "#### Balans klas zbioru o stałym aspekcie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9b3ff-d4bf-4514-815b-4cb050522227",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([same_aspect_df.value_counts('class'), same_aspect_df.value_counts('class', normalize=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fc9887-7732-4be4-8960-895e62c15c6a",
   "metadata": {},
   "source": [
    "Niestety widzimy, że tym samym straciliśmy zupełnie jedynastą klasę, a kolejne cztery mają liczebność mniejszą niż klasa jedynasta przed redukcją zbioru. W tym miejscu powinniśmy zadać sobie pytanie, czy klasy te są kluczowe dla rozwiązania problemu.\n",
    "\n",
    "Jeśli są, to albo musimy poluzować założenia na temat wymiarów i aspektu albo popracować nad zbiorem danych. Możliwe, że w danych, które odrzuciliśmy są obrazy, które bez utraty zaznaczonych obiektów można dociąć do oczekiwanego przez nas aspektu. \n",
    "\n",
    "W przeciwnym razie konieczne będzie rozszerzenie zbioru danych kolejnymi ręcznie oznaczanymi przykładami."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9b242-f1bf-4ddb-b890-a08bd0637962",
   "metadata": {},
   "source": [
    "### Zaznaczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf9b5e-3c95-4144-bce5-42d47c2d1cc4",
   "metadata": {},
   "source": [
    "Sprawdźmy zaznaczenia i czy ich rozkłady zmieniły się po ograniczeniu zbioru.\n",
    "Na początek cały zbiór."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9cac98-bd6f-4617-8093-35ca27f2fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[:, ['x_box_center', 'y_box_center', 'box_width', 'box_height']].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a44094-530d-4073-a6ac-892f6bbb6c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_aspect_df.loc[:, ['x_box_center', 'y_box_center', 'box_width', 'box_height']].hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c2c33-876e-4d3b-89b7-55a34ee99e3c",
   "metadata": {},
   "source": [
    "Widzmimy, że rozkłady nie różnią się znacznie. Dla współrzędnych środka widzimy, że zaznaczenia rzadko pojawiają się na środku, ale za to mają dwie mody każda skoncentrowana wokół 0.25 i 0.75 względnego rozmiaru obrazu.\n",
    "\n",
    "Rozmiary zaznaczeń są w większości znacznie mniejsze od rozmiaru samego obrazu, więc jest szansa, że rzeczywiście te z innymi aspektami dałoby się dosztukować do aspektu HD.\n",
    "\n",
    "Widzimy także pewne artefakty na tych histogramach, które prawdopodobnie świadczą o tym, że znaczna część zaznaczeń wraz z odpowiadającymi im obrazami pochodzi z tego podobnego procesu, gdzie albo obiekty występują w konkretnym miejscu, albo jest to seria klatek z okresu o niedużej zmienności przedstawianego otoczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4262abb-df82-438f-9191-c66918721bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.x_box_center.apply(lambda x: round(x, 2)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a93c649-48a4-48dd-9da9-6e3979113116",
   "metadata": {},
   "source": [
    "Widzmimy faktycznie dużo próbek dla wartości współrzędnej poziomej o wartości 0.41-0.43."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe2cc4-fae0-4752-81de-06626336446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.loc[(full_df.x_box_center <0.44) & (full_df.x_box_center > 0.4)].image_name.apply(lambda x: x.split('_')[0][:5]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ad3e06-4307-44ff-902c-3ae6d1bf19c3",
   "metadata": {},
   "source": [
    "Widzimy po nazwach plików, że faktycznie znaczna część z obrazów, które mają te wartości położeń zaznaczeń, ma nazwy zaczynające się podobnie. Nadmierna liczba próbek z tego samego procesu może mieć silną korelację, co może prowadzić do problemów z generalizacją modeli, które nauczą się cech tła zamiast cech obiektów, które mają wykrywać."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad65e3-f321-4fbf-a94d-00c09f7fb9fd",
   "metadata": {},
   "source": [
    "#### Liczba zanzaczeń\n",
    "\n",
    "Możemy jeszcze zobaczyć, jak rozkłada się liczba zaznaczeń per obraz w obu wersjach zbioru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99f9ae1-d74a-4056-8da7-fb113a09b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.image_name.value_counts().value_counts().plot(style='.')\n",
    "plt.xticks(range(int(1), 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aec60c-dc56-4c83-94da-71ce33301bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_aspect_df.image_name.value_counts().value_counts().plot(style='.')\n",
    "plt.xticks(range(int(1), 14))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245fab3d-8df9-4772-95a9-96c4da12a42e",
   "metadata": {},
   "source": [
    "Widzimy, że większość obrazów rzeczywiście ma niewiele zaznaczeń 1-5. Co ciekawe odsiewając obrazy o innych aspektach straciliśmy te, które miały bardzo duże liczby zaznaczeń, co może być przypadkiem brzegowym, który i tak byłby trudno dla modelu do rozpoznania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a6c74-fa46-4309-9997-19a119d8e306",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "1. Głównym problemem zbioru danych jest niezbalansowanie klas.\n",
    "2. Część obrazów jest w nietypowej rozdzielczości. Mają inne aspekt albo bardzo małe lub duże wymiary.\n",
    "3. Po odsianiu nietypowych obrazów niezbalansowanie klas się pogłębia.\n",
    "\n",
    "  #### Co można zrobić?\n",
    "1. Trzeba przejrzeć dostępne modele i zrewidować, czy problem balansu klas, wymiarów lub aspektu jest istotny.\n",
    "2. Edytować nietypowe obrazy, by przy pomocy ręcznych narzędzi cięcia i skalowania ustandaryzować je.\n",
    "3. W skrajnym przypadku poszerzyć zbiór o nowe ręcznie oznaczane próbki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3b6298-9a00-4168-8eb0-1f3e144eb308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
